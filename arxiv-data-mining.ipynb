{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse von Arxiv: Strukturierte und unstrukturierte Daten\n",
    "\n",
    "Dieses Jupyter Notebook stellt die Prüfungsaufgabe des Moduls \"Maschinelles Lernen\" (ROB60) der Akad University dar.\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Die Aufgabenstellung dieser Prüfung ist es Artikel zu einem selbstgewählten Thema der Webseite [Arxiv](https://arxiv.org/) zu analysieren.\n",
    "Im ersten Schritt wird ein Histogramm der über die Anzahl der Artikel pro Autor erstellt. Im Weiteren wird eine deskriptive Analyse für die verschiedenen Jahr durchgeführt.\n",
    "Das Ziel ist zu eruieren, ob sich über die Jahre gewisse Parameter oder Korrelationen verändert haben. Dies könnte beispielsweise die Anzahl der veröffentlichten Artikel pro Autor\n",
    "oder die Anzahl an Autoren sein. Darüber hinaus wird eine Textanalyse durchgeführt. Diese Analyse wird anhand des Abstracts der Artikel durchgeführt.\n",
    "Diese Aufgaben werden sich am CRISP DM Standard orientieren und die folgenden Punkte beinhalten:\n",
    "\n",
    "1. Verstehen der Daten (Data Understanding)\n",
    "2. Datenvorbereitung (Data Preparation)\n",
    "3. Modellierung (Modeling)\n",
    "4. Auswertung (Evaluation)\n",
    "\n",
    "## Extraktion und Verstehen der Daten (Data Understanding)\n",
    "\n",
    "Für die Extraktion der Daten findet mittels der Web-API der Platform von Arxiv statt. Es wird das exemplarische Stichwort \"Deep Reinforcement Learning\" gewählt.\n",
    "Um eine möglichst aussagekräftige Analyse durchführen zu können, werden Artikel für die Jahre 2017-2021 heruntergeladen. Pro Jahr sollen 200 Artikel heruntergeladen und in die\n",
    "Analyse miteinbezogen werden.\n",
    "\n",
    "### Beschreibung der Datenquelle und des Codes zur Extraktion\n",
    "\n",
    "Die Platform Arxiv bietet eine Web-Api an, um Metadaten der Artikel zu extrahieren. Hierbei wird eine Anfrage an den Server mittels des Protokolls gesendet.\n",
    "Die Parametrisierung findet mittels der URL statt. Für die Extraktion eben jener Daten wird das Paket `arxiv` verwendet. Es bietet eine API zur Parametrisierung der Anfrage an.\n",
    "Im folgenden Code wird zunächst ein leeres Array für die extrahierten Artikel angelegt. Darüber hinaus werden die Jahreszahlen in einem Array definiert und die maximale Anzahl an Artikeln\n",
    "pro Jahr. Aufgrund einer Limitation der Api, die nur maximal 200 Artikel pro Anfrage zurückliefert, werden mehrere Iterationen durchgeführt. Für jedes Jahr wird eine Suche durchgeführt und\n",
    "die Metadaten Titel, Autoren, Zusammenfassung und Veröffentlichungsdatum der einzelnen Artikel einem Array hinzugefügt. Zuletzt werden die Artikel in einem Dataframe gespeichert.\n"
   ],
   "id": "3eaccebc836882e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T18:39:48.435926Z",
     "start_time": "2025-01-27T18:39:17.143740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from arxiv import SortCriterion, Search\n",
    "\n",
    "# Variables\n",
    "articles = []\n",
    "years = [2018, 2019, 2020, 2021]\n",
    "number_of_articles = 200\n",
    "client = arxiv.Client()\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Define search query\n",
    "    search_query = f\"Deep Reinforcement Learning AND submittedDate:[{year}01010000 TO {year}12312359]\"\n",
    "\n",
    "    # Search for the articles\n",
    "    search = Search(\n",
    "        query=search_query,\n",
    "        max_results=number_of_articles,\n",
    "    )\n",
    "\n",
    "    # Request the articles\n",
    "    results = client.results(search)\n",
    "\n",
    "    # Save retrieved data in array\n",
    "    for result in results:\n",
    "        article = {\n",
    "            \"Title\": result.title,\n",
    "            \"Authors\": [str(author) for author in result.authors],\n",
    "            \"Abstract\": result.summary,\n",
    "            \"PublishedDate\": result.published.date(),\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "# Save articles in data frame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Print types and head\n",
    "print(f\"Types of the data frame \\n {df.dtypes}\")\n",
    "print(f\"The head of the data frame \\n {df.head()}\")"
   ],
   "id": "eab8a0410836482f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of the data frame \n",
      " Title            object\n",
      "Authors          object\n",
      "Abstract         object\n",
      "PublishedDate    object\n",
      "dtype: object\n",
      "The head of the data frame \n",
      "                                                Title  \\\n",
      "0  Applications of Deep Reinforcement Learning in...   \n",
      "1           Generalization and Regularization in DQN   \n",
      "2       Object-sensitive Deep Reinforcement Learning   \n",
      "3  Towards Symbolic Reinforcement Learning with C...   \n",
      "4  Exploration by Distributional Reinforcement Le...   \n",
      "\n",
      "                                             Authors  \\\n",
      "0  [Nguyen Cong Luong, Dinh Thai Hoang, Shimin Go...   \n",
      "1  [Jesse Farebrother, Marlos C. Machado, Michael...   \n",
      "2            [Yuezhang Li, Katia Sycara, Rahul Iyer]   \n",
      "3  [Artur d'Avila Garcez, Aimore Resende Riquetti...   \n",
      "4                      [Yunhao Tang, Shipra Agrawal]   \n",
      "\n",
      "                                            Abstract PublishedDate  \n",
      "0  This paper presents a comprehensive literature...    2018-10-18  \n",
      "1  Deep reinforcement learning algorithms have sh...    2018-09-29  \n",
      "2  Deep reinforcement learning has become popular...    2018-09-17  \n",
      "3  Deep Reinforcement Learning (deep RL) has made...    2018-04-23  \n",
      "4  We propose a framework based on distributional...    2018-05-04  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vorbereiten der Daten (Data Preparation)\n",
    "\n",
    "Im folgenden Abschnitt wird eine Aufbereitung der Daten vorgenommen. Dies ist ein essenzieller Teil jedes Datenprojektes. Je besser die Daten aufbereitet wurden, desto aussagekräftiger\n",
    "sind die Ergebnisse der nachfolgenden Analyse des Datenbestands. Sie wird unmittelbar nach der Extraktion der Daten vorgenommen. Hierzu gehören üblicherweise folgende Schritte:\n",
    "\n",
    "1. Umbenennung der Spalten für eine aussagekräftigere Darstellung der Daten und des verarbeitenden Codes.\n",
    "2. Entfernung irrelevanter Spalten für die weitere Verarbeitung\n",
    "3. Entfernung von Duplikaten und Zeilen mit Null-Werten.\n",
    "4. Entfernung von Ausreißern\n",
    "\n",
    "Aus der Codezelle zur Datenextraktion geht hervor, dass eine Selektion und Benennung der Spalten für das DataFrame bereits stattgefunden hat. Die ersten beiden Schritte wurden somit durchgeführt.\n",
    "Eine Bereinigung von Ausreißern findet in diesem DataFrame nicht statt, da außer dem Veröffentlichungsdatum keine numerischen Werte enthalten sind. Eine Bereinigung von Ausreißern wäre hier auch nicht nötig,\n",
    "da die Jahreszahlen explizit bei der Abfrage parametrisiert wurden.\n",
    "\n",
    "In der folgenden Codezelle wird nach der Extraktion eine Bereinigung der Werte hinsichtlich vorkommender Null-Werte vorgenommen. Hierzu wird die Methode `dropna` auf das DataFrame angewendet.\n",
    "Zudem wird die Spalte `Authors` expandiert. Viele Artikel haben mehrere Autoren. Diese wurden im DataFrame als `list` gespeichert. Damit die Analyse hinsichtlich der Anzahl veröffentlichter Publikationen pro Autor durchgeführt werden kann,\n",
    "ist es sinnvoll mit der Methode `explode` eine Expansion durchzuführen. Diese Operation wird nachfolgend exemplarisch dargestellt.\n",
    "\n",
    "Ausgehend von der folgenden Tabelle in denen exemplarisch zwei Publikationen von mehreren Autoren verfasst wurden, sollen für jeden Autor weitere Datensätze erzeugt werden.\n",
    "Die anderen Werte sollen dabei nicht verändert werden\n",
    "\n",
    "| Titel   | Autoren            | Veröffentlichungsdatum |\n",
    "|---------|--------------------|------------------------|\n",
    "| Titel A | [Autor A, Autor B] | 2018                   |\n",
    "| Titel B | [Autor B, Autor C] | 2019                   |\n",
    "\n",
    "Das Ergebnis der Methode `explode` würde wie folgt aussehen:\n",
    "\n",
    "| Titel   | Autoren | Veröffentlichungsdatum |\n",
    "|---------|---------|------------------------|\n",
    "| Titel A | Autor A | 2018                   |\n",
    "| Titel A | Autor B | 2018                   |\n",
    "| Titel B | Autor B | 2019                   |\n",
    "| Titel B | Autor C | 2019                   |\n",
    "\n",
    "Mithilfe der zweiten Tabelle kann nun eine Quantifizierung der Autoren pro Publikation durchgeführt werden.\n",
    "Hierzu wird über die definierten Jahre iteriert und das DataFrame für das entsprechende Jahr gefiltert. Dies wird mit der `isin`-Methode getan.\n",
    "Die zehn Autoren mit den meisten geschrieben Publikationen des entsprechenden Jahres werden auf der Spalte mit der Methode `value_counts` erhoben.\n",
    "Eine Limitierung auf die 10 Autoren mit den meisten Publikation wird mit der Methode `head` durchgeführt.\n",
    "Anschließend wird das Histogramm dargestellt. Die Autoren werden auf der x-Achse dargestellt. Für jeden Autor gibt es einen Balken.\n",
    "Auf der y-Achse erweist sich eine Skala von 0 bis 10 als sinnvoll. Die Schrittweite beträgt 1."
   ],
   "id": "a5b0534e86fda25c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from extraction import extract_data_from_arxiv\n",
    "import pandas as pd\n",
    "\n",
    "# Extract data from API\n",
    "df = extract_data_from_arxiv()\n",
    "\n",
    "# Ensure PublishedDate is in datetime format\n",
    "df[\"PublishedDate\"] = pd.to_datetime(df[\"PublishedDate\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with null values in Authors or PublishedDate\n",
    "df = df.dropna(subset=[\"Authors\", \"PublishedDate\"])\n",
    "\n",
    "# Expand the possible multiple authors of a paper to an own entry for each author\n",
    "df = df.explode(column=\"Authors\")\n",
    "\n",
    "# Define the years to analyze\n",
    "years = range(2018, 2022)\n",
    "\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Skip the year if no data is found\n",
    "    if year_filtered_df.empty:\n",
    "        print(f\"No data found for year {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Count authors for the selected year and take the top 10 authors with the most publications\n",
    "    top_author_counts = year_filtered_df[\"Authors\"].value_counts().head(10)\n",
    "\n",
    "    # Plot the histogram\n",
    "    top_author_counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Top 10 authors for {year}\")\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, max(top_author_counts.max() + 1, 11), 1))\n",
    "    plt.show()"
   ],
   "id": "cfb3d6d51c876dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from calendar import month_name\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "\n",
    "# Define years and the months order\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Get a mapping of month names to their numerical values\n",
    "month_order = {month: i for i, month in enumerate(month_name) if month}\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False)\n",
    "\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Skip the year if no data is found\n",
    "    if year_filtered_df.empty:\n",
    "        print(f\"No data found for year {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get number of articles per month and sort the series by the index\n",
    "    articles_per_month = year_filtered_df[\"PublishedDate\"].groupby(year_filtered_df[\"PublishedDate\"].dt.month_name()).count()\n",
    "    sorted_articles_per_month = articles_per_month.sort_index(key=lambda x: x.map(month_order))\n",
    "\n",
    "    # Plot the histogram\n",
    "    sorted_articles_per_month.plot(kind=\"bar\")\n",
    "    plt.title(f\"Number of articles per month for {year}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, max(sorted_articles_per_month.max() + 1, 11), 10))\n",
    "    plt.show()"
   ],
   "id": "fd5ebb9e0c30de06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from calendar import month_name\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "\n",
    "# Define years and the months order\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Get a mapping of month names to their numerical values\n",
    "month_order = {month: i for i, month in enumerate(month_name) if month}\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False)\n",
    "\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Skip the year if no data is found\n",
    "    if year_filtered_df.empty:\n",
    "        print(f\"No data found for year {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get number of authors per month and sort the series by the index\n",
    "    authors_per_month = (year_filtered_df.groupby(year_filtered_df[\"PublishedDate\"].dt.month_name())[\"Authors\"]\n",
    "                                         .nunique()\n",
    "                                         .reindex(month_order, fill_value=0))\n",
    "\n",
    "    # Plot the histogram\n",
    "    authors_per_month.plot(kind=\"bar\")\n",
    "    plt.title(f\"Number of authors per month for {year}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, max(authors_per_month.max() + 1, 11), 10))\n",
    "    plt.show()"
   ],
   "id": "9f25216f8627963c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
