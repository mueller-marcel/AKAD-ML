{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse von Arxiv: Strukturierte und unstrukturierte Daten\n",
    "\n",
    "Dieses Jupyter Notebook stellt die Prüfungsaufgabe des Moduls \"Maschinelles Lernen\" (ROB60) der Akad University dar.\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Die Aufgabenstellung dieser Prüfung ist es Artikel zu einem selbstgewählten Thema der Webseite [Arxiv](https://arxiv.org/) zu analysieren.\n",
    "Im ersten Schritt wird ein Histogramm der über die Anzahl der Artikel pro Autor erstellt. Im Weiteren wird eine deskriptive Analyse für die verschiedenen Jahr durchgeführt.\n",
    "Das Ziel ist zu eruieren, ob sich über die Jahre gewisse Parameter oder Korrelationen verändert haben. Dies könnte beispielsweise die Anzahl der veröffentlichten Artikel pro Autor\n",
    "oder die Anzahl an Autoren sein. Darüber hinaus wird eine Textanalyse durchgeführt. Diese Analyse wird anhand des Abstracts der Artikel durchgeführt.\n",
    "Diese Aufgaben werden sich am CRISP DM Standard orientieren und die folgenden Punkte beinhalten:\n",
    "\n",
    "1. Verstehen der Daten (Data Understanding)\n",
    "2. Datenvorbereitung (Data Preparation)\n",
    "3. Modellierung (Modeling)\n",
    "4. Auswertung (Evaluation)\n",
    "\n",
    "## Extraktion und Verstehen der Daten (Data Understanding)\n",
    "\n",
    "Für die Extraktion der Daten findet mittels der Web-API der Platform von Arxiv statt. Es wird das exemplarische Stichwort \"Deep Reinforcement Learning\" gewählt.\n",
    "Um eine möglichst aussagekräftige Analyse durchführen zu können, werden Artikel für die Jahre 2017-2021 heruntergeladen. Pro Jahr sollen 200 Artikel heruntergeladen und in die\n",
    "Analyse miteinbezogen werden.\n",
    "\n",
    "### Beschreibung der Datenquelle und des Codes zur Extraktion\n",
    "\n",
    "Die Platform Arxiv bietet eine Web-Api an, um Metadaten der Artikel zu extrahieren. Hierbei wird eine Anfrage an den Server mittels des Protokolls gesendet.\n",
    "Die Parametrisierung findet mittels der URL statt. Für die Extraktion eben jener Daten wird das Paket `arxiv` verwendet. Es bietet eine API zur Parametrisierung der Anfrage an.\n",
    "Im folgenden Code wird zunächst ein leeres Array für die extrahierten Artikel angelegt. Darüber hinaus werden die Jahreszahlen in einem Array definiert und die maximale Anzahl an Artikeln\n",
    "pro Jahr. Aufgrund einer Limitation der Api, die nur maximal 200 Artikel pro Anfrage zurückliefert, werden mehrere Iterationen durchgeführt. Für jedes Jahr wird eine Suche durchgeführt und\n",
    "die Metadaten Titel, Autoren, Zusammenfassung und Veröffentlichungsdatum der einzelnen Artikel einem Array hinzugefügt. Zuletzt werden die Artikel in einem Dataframe gespeichert.\n"
   ],
   "id": "3eaccebc836882e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from arxiv import Search\n",
    "\n",
    "# Variables\n",
    "articles = []\n",
    "years = [2018, 2019, 2020, 2021]\n",
    "number_of_articles = 200\n",
    "client = arxiv.Client()\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Define search query\n",
    "    search_query = f\"Deep Reinforcement Learning AND submittedDate:[{year}01010000 TO {year}12312359]\"\n",
    "\n",
    "    # Search for the articles\n",
    "    search = Search(\n",
    "        query=search_query,\n",
    "        max_results=number_of_articles,\n",
    "    )\n",
    "\n",
    "    # Request the articles\n",
    "    results = client.results(search)\n",
    "\n",
    "    # Save retrieved data in array\n",
    "    for result in results:\n",
    "        article = {\n",
    "            \"Title\": result.title,\n",
    "            \"Authors\": [str(author) for author in result.authors],\n",
    "            \"Abstract\": result.summary,\n",
    "            \"PublishedDate\": result.published.date(),\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "# Save articles in data frame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Print types and head\n",
    "print(f\"Types of the data frame \\n {df.dtypes}\")\n",
    "print(f\"The head of the data frame \\n {df.head()}\")"
   ],
   "id": "eab8a0410836482f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vorbereiten der Daten (Data Preparation)\n",
    "\n",
    "Im folgenden Abschnitt wird eine Aufbereitung der Daten vorgenommen. Dies ist ein essenzieller Teil jedes Datenprojektes. Je besser die Daten aufbereitet wurden, desto aussagekräftiger\n",
    "sind die Ergebnisse der nachfolgenden Analyse des Datenbestands. Sie wird unmittelbar nach der Extraktion der Daten vorgenommen. Hierzu gehören üblicherweise folgende Schritte:\n",
    "\n",
    "1. Umbenennung der Spalten für eine aussagekräftigere Darstellung der Daten und des verarbeitenden Codes.\n",
    "2. Entfernung irrelevanter Spalten für die weitere Verarbeitung\n",
    "3. Entfernung von Duplikaten und Zeilen mit Null-Werten.\n",
    "4. Entfernung von Ausreißern\n",
    "\n",
    "Aus der Codezelle zur Datenextraktion geht hervor, dass eine Selektion und Benennung der Spalten für das DataFrame bereits stattgefunden hat. Die ersten beiden Schritte wurden somit durchgeführt.\n",
    "Eine Bereinigung von Ausreißern findet in diesem DataFrame nicht statt, da außer dem Veröffentlichungsdatum keine numerischen Werte enthalten sind. Eine Bereinigung von Ausreißern wäre hier auch nicht nötig,\n",
    "da die Jahreszahlen explizit bei der Abfrage parametrisiert wurden.\n",
    "\n",
    "In der folgenden Codezelle wird nach der Extraktion eine Bereinigung der Werte hinsichtlich vorkommender Null-Werte vorgenommen. Hierzu wird die Methode `dropna` auf das DataFrame angewendet.\n",
    "Zudem wird die Spalte `Authors` expandiert. Viele Artikel haben mehrere Autoren. Diese wurden im DataFrame als `list` gespeichert. Damit die Analyse hinsichtlich der Anzahl veröffentlichter Publikationen pro Autor durchgeführt werden kann,\n",
    "ist es sinnvoll mit der Methode `explode` eine Expansion durchzuführen. Diese Operation wird nachfolgend exemplarisch dargestellt.\n",
    "\n",
    "Ausgehend von der folgenden Tabelle in denen exemplarisch zwei Publikationen von mehreren Autoren verfasst wurden, sollen für jeden Autor weitere Datensätze erzeugt werden.\n",
    "Die anderen Werte sollen dabei nicht verändert werden\n",
    "\n",
    "| Titel   | Autoren            | Veröffentlichungsdatum |\n",
    "|---------|--------------------|------------------------|\n",
    "| Titel A | [Autor A, Autor B] | 2018                   |\n",
    "| Titel B | [Autor B, Autor C] | 2019                   |\n",
    "\n",
    "Das Ergebnis der Methode `explode` würde wie folgt aussehen:\n",
    "\n",
    "| Titel   | Autoren | Veröffentlichungsdatum |\n",
    "|---------|---------|------------------------|\n",
    "| Titel A | Autor A | 2018                   |\n",
    "| Titel A | Autor B | 2018                   |\n",
    "| Titel B | Autor B | 2019                   |\n",
    "| Titel B | Autor C | 2019                   |\n",
    "\n",
    "Mithilfe der zweiten Tabelle kann nun eine Quantifizierung der Autoren pro Publikation durchgeführt werden.\n",
    "Hierzu wird über die definierten Jahre iteriert und das DataFrame für das entsprechende Jahr gefiltert. Dies wird mit der `isin`-Methode getan.\n",
    "Die zehn Autoren mit den meisten geschrieben Publikationen des entsprechenden Jahres werden auf der Spalte mit der Methode `value_counts` erhoben.\n",
    "Eine Limitierung auf die 10 Autoren mit den meisten Publikation wird mit der Methode `head` durchgeführt.\n",
    "Anschließend wird das Histogramm dargestellt. Die Autoren werden auf der x-Achse dargestellt. Für jeden Autor gibt es einen Balken.\n",
    "Auf der y-Achse erweist sich eine Skala von 0 bis 10 als sinnvoll. Die Schrittweite beträgt 1."
   ],
   "id": "a5b0534e86fda25c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from extraction import extract_data_from_arxiv\n",
    "import pandas as pd\n",
    "\n",
    "# Extract data from API\n",
    "df = extract_data_from_arxiv()\n",
    "\n",
    "# Ensure PublishedDate is in datetime format\n",
    "df[\"PublishedDate\"] = pd.to_datetime(df[\"PublishedDate\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with null values in Authors or PublishedDate\n",
    "df = df.dropna(subset=[\"Authors\", \"PublishedDate\"])\n",
    "\n",
    "# Expand the possible multiple authors of a paper to an own entry for each author\n",
    "df = df.explode(column=\"Authors\")\n",
    "\n",
    "# Define the years to analyze\n",
    "years = range(2018, 2022)\n",
    "\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Skip the year if no data is found\n",
    "    if year_filtered_df.empty:\n",
    "        print(f\"No data found for year {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Count authors for the selected year and take the top 10 authors with the most publications\n",
    "    top_author_counts = year_filtered_df[\"Authors\"].value_counts().head(10)\n",
    "\n",
    "    # Plot the histogram\n",
    "    top_author_counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Top 10 authors for {year}\")\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, max(top_author_counts.max() + 1, 11), 1))\n",
    "    plt.show()"
   ],
   "id": "cfb3d6d51c876dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Darstellung der Artikel pro Monat in einem Histogramm",
   "id": "170f62ecb6eb8ae5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from calendar import month_name\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "\n",
    "# Define years and the months order\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Get a mapping of month names to their numerical values\n",
    "month_order = {month: i for i, month in enumerate(month_name) if month}\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False, explode_authors=True)\n",
    "\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Skip the year if no data is found\n",
    "    if year_filtered_df.empty:\n",
    "        print(f\"No data found for year {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get number of articles per month and sort the series by the index\n",
    "    articles_per_month = (year_filtered_df[\"PublishedDate\"].groupby(year_filtered_df[\"PublishedDate\"].dt.month_name())\n",
    "                          .count()\n",
    "                          .reindex(month_order, fill_value=0))\n",
    "\n",
    "    # Plot the histogram\n",
    "    articles_per_month.plot(kind=\"bar\")\n",
    "    plt.title(f\"Number of articles per month for {year}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, max(articles_per_month.max() + 1, 11), 10))\n",
    "    plt.show()"
   ],
   "id": "fd5ebb9e0c30de06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Darstellung der Anzahl der Autoren pro Monat in einem Histogramm",
   "id": "fd60c70c8ef9e7f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from calendar import month_name\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "\n",
    "# Define years and the months order\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Get a mapping of month names to their numerical values\n",
    "month_order = {month: i for i, month in enumerate(month_name) if month}\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False, explode_authors=True)\n",
    "\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Skip the year if no data is found\n",
    "    if year_filtered_df.empty:\n",
    "        print(f\"No data found for year {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get number of authors per month and sort the series by the index\n",
    "    authors_per_month = (year_filtered_df.groupby(year_filtered_df[\"PublishedDate\"].dt.month_name())[\"Authors\"]\n",
    "                         .nunique()\n",
    "                         .reindex(month_order, fill_value=0))\n",
    "\n",
    "    # Plot the histogram\n",
    "    authors_per_month.plot(kind=\"bar\")\n",
    "    plt.title(f\"Number of authors per month for {year}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, max(authors_per_month.max() + 1, 11), 10))\n",
    "    plt.show()"
   ],
   "id": "9f25216f8627963c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyse der Keywords des Titels in einer Wordcloud",
   "id": "5a1154d51508399e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "\n",
    "# Define the relevant years for the analysis\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False, explode_authors=True)\n",
    "\n",
    "\n",
    "# Clean the titles from special chars and lower the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Clean the title from special characters\n",
    "    cleaned_df = pd.DataFrame(year_filtered_df[\"Title\"].apply(clean_text))\n",
    "\n",
    "    # Set the stop words\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        stopwords=stopwords,\n",
    "        max_words=500,\n",
    "        max_font_size=100,\n",
    "        random_state=100,\n",
    "    ).generate(str(cleaned_df))\n",
    "\n",
    "    # Print the wordcloud\n",
    "    fig = plt.figure(1)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Identify topics in the titles of the articles using the Latent Dirichlet allocation\n",
    "    vectorizer = CountVectorizer(\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "    )\n",
    "    transformed_vectorizer = vectorizer.fit_transform(cleaned_df[\"Title\"])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Tries to create combinations from the titles regarding multiple topics. The hyperparameters are the number of components,\n",
    "    # the maximum iterations and the learning method\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=20,\n",
    "        max_iter=5,\n",
    "        learning_method='online',\n",
    "        learning_offset=50,\n",
    "        random_state=0,\n",
    "    ).fit(transformed_vectorizer)\n",
    "\n",
    "    # Print topics\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print(f\"Thema: {topic_idx}\")\n",
    "        print([feature_names[i] for i in topic.argsort()[:-5 - 1:-1]])"
   ],
   "id": "9da9018be873f937",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Darstellung der Länge des Abstracts in einem Boxplot",
   "id": "7e2b37c6decc047f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "\n",
    "# Define the years to analyze\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False, explode_authors=True)\n",
    "\n",
    "\n",
    "# Define a function that removes the spaces from the abstract and calculates the length\n",
    "def calculate_trimmed_length(text: str):\n",
    "    return len(text.replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "# Sort the dataframe by the values\n",
    "df = df.sort_values(by=[\"PublishedDate\"])\n",
    "\n",
    "# Calculate the length of the abstract\n",
    "df[\"abstract_length\"] = df[\"Abstract\"].apply(calculate_trimmed_length)\n",
    "\n",
    "# Print boxplot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxenplot(y=df[\"abstract_length\"], color=\"skyblue\")\n",
    "plt.ylabel(\"Länge des Abstracts in Anzahl Zeichen\")\n",
    "plt.title(\"Boxplot der Abstract-Längen\")\n",
    "plt.show()"
   ],
   "id": "96cf0cb8e6d0bc02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Häufigkeitsanalyse von Themen der Abstracts für jedes Jahr",
   "id": "57ee7b5d4171f523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from spacy.cli import download\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "from collections import Counter\n",
    "\n",
    "# Define the years to analyze\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Extract and prepare data\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False, explode_authors=False)\n",
    "\n",
    "# Download and load the language model for the stopwords removal and the lemmatization\n",
    "download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Define a function to be applied on the Abstract column for the text preparation\n",
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_words = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "# Apply the text preparation on the abstract column\n",
    "df[\"Abstract\"] = df[\"Abstract\"].apply(clean_text)\n",
    "\n",
    "# Iterate over years and create a bar chart for every year\n",
    "for year in years:\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Collect all words from the abstracts\n",
    "    all_words = \" \".join(year_filtered_df[\"Abstract\"]).split()\n",
    "\n",
    "    # Calculate the word frequency for every word and take the 10 most common words\n",
    "    word_frequency = Counter(all_words)\n",
    "    top_words = word_frequency.most_common(10)\n",
    "\n",
    "    # Create a dataframe from the words and the frequencies\n",
    "    word_df = pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])\n",
    "\n",
    "    # Print the bar chart\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=\"Frequency\", y=\"Word\", data=word_df)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.title(f\"Top 10 most frequent words in abstracts in {year}\")\n",
    "    plt.show()"
   ],
   "id": "4e4e37e9af7ed5fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TF-IDF",
   "id": "f7b7cbeeed1d3f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from extraction import extract_data_from_arxiv\n",
    "from preparation import prepare_data\n",
    "from filtering import filter_abstract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Find best K with elbow method to cluster\n",
    "def find_optimal_k(x, max_k=10):\n",
    "    distortions = []\n",
    "    for k in range(1, max_k):\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        km.fit(x)\n",
    "        distortions.append(km.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, max_k), distortions, marker='o')\n",
    "    plt.xlabel(\"Anzahl der Cluster\")\n",
    "    plt.ylabel(\"Trägheit (Inertia)\")\n",
    "    plt.title(\"Elbow-Methode zur Bestimmung von K\")\n",
    "    plt.show()\n",
    "\n",
    "    # Get keywords from the clusters\n",
    "def get_cluster_keywords(x, clusters, features, top_n=5):\n",
    "    cluster_keywords = {}\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_indices = np.where(clusters == cluster)[0]\n",
    "        words = np.array(x[cluster_indices].sum(axis=0)).flatten()\n",
    "        most_significant_words = [features[i] for i in words.argsort()[-top_n:][::-1]]\n",
    "        cluster_keywords[cluster] = \", \".join(most_significant_words)\n",
    "    return cluster_keywords\n",
    "\n",
    "\n",
    "# Define the years to analyze\n",
    "years = range(2018, 2022)\n",
    "\n",
    "# Extract and prepare data and filter the abstract for stopwords\n",
    "df = extract_data_from_arxiv()\n",
    "df = prepare_data(df, years, show_histograms=False, explode_authors=False)\n",
    "df = filter_abstract(df)\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Filter for the specific year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].dt.year == year]\n",
    "\n",
    "    # Vectorize the abstract\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(df[\"Abstract\"])\n",
    "\n",
    "    # Add delimiter to separate the years in the output\n",
    "    print(f\"===================================================   {year}   ===================================================\")\n",
    "\n",
    "    # Apply Kmeans algorithm\n",
    "    find_optimal_k(X, max_k=10)\n",
    "\n",
    "    # Use elbow method to cluster\n",
    "    num_clusters = 5\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    df[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "    # Get names of clusters\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    cluster_names = get_cluster_keywords(X.toarray(), df[\"Cluster\"].values, feature_names)\n",
    "\n",
    "    # Show results with cluster names\n",
    "    df[\"Cluster Name\"] = df[\"Cluster\"].map(cluster_names)\n",
    "\n",
    "    # Print wordcloud for the cluster names\n",
    "    fig, axes = plt.subplots(1, len(cluster_names), figsize=(15, 5))\n",
    "    for i, (cluster_id, keywords) in enumerate(cluster_names.items()):\n",
    "        wordcloud = WordCloud(background_color=\"white\").generate(keywords)\n",
    "        axes[i].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        axes[i].set_title(f\"Cluster {cluster_id}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ],
   "id": "a7b755e94c9e3cc1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
