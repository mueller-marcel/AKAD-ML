{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse von Arxiv: Strukturierte und unstrukturierte Daten\n",
    "\n",
    "Dieses Jupyter Notebook stellt die Prüfungsaufgabe des Moduls \"Maschinelles Lernen\" (ROB60) der Akad University dar.\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Die Aufgabenstellung dieser Prüfung ist es Artikel zu einem selbstgewählten Thema der Webseite [Arxiv](https://arxiv.org/) zu analysieren.\n",
    "Im ersten Schritt wird ein Histogramm der über die Anzahl der Artikel pro Autor erstellt. Im Weiteren wird eine deskriptive Analyse für die verschiedenen Jahr durchgeführt.\n",
    "Das Ziel ist zu eruieren, ob sich über die Jahre gewisse Parameter oder Korrelationen verändert haben. Dies könnte beispielsweise die Anzahl der veröffentlichten Artikel pro Autor\n",
    "oder die Anzahl an Autoren sein. Darüber hinaus wird eine Textanalyse durchgeführt. Diese Analyse wird anhand des Abstracts der Artikel durchgeführt.\n",
    "Diese Aufgaben werden sich am CRISP DM Standard orientieren und die folgenden Punkte beinhalten:\n",
    "\n",
    "1. Verstehen der Daten (Data Understanding)\n",
    "2. Datenvorbereitung (Data Preparation)\n",
    "3. Modellierung (Modeling)\n",
    "4. Auswertung (Evaluation)\n",
    "\n",
    "## Extraktion und Verstehen der Daten (Data Understanding)\n",
    "\n",
    "Für die Extraktion der Daten findet mittels der Web-API der Platform von Arxiv statt. Es wird das exemplarische Stichwort \"Deep Reinforcement Learning\" gewählt.\n",
    "Um eine möglichst aussagekräftige Analyse durchführen zu können, werden Artikel für die Jahre 2017-2021 heruntergeladen. Pro Jahr sollen 200 Artikel heruntergeladen und in die\n",
    "Analyse miteinbezogen werden.\n",
    "\n",
    "### Beschreibung der Datenquelle und des Codes zur Extraktion\n",
    "\n",
    "Die Platform Arxiv bietet eine Web-Api an, um Metadaten der Artikel zu extrahieren. Hierbei wird eine Anfrage an den Server mittels des Protokolls gesendet.\n",
    "Die Parametrisierung findet mittels der URL statt. Für die Extraktion eben jener Daten wird das Paket `arxiv` verwendet. Es bietet eine API zur Parametrisierung der Anfrage an.\n",
    "Im folgenden Code wird zunächst ein leeres Array für die extrahierten Artikel angelegt. Darüber hinaus werden die Jahreszahlen in einem Array definiert und die maximale Anzahl an Artikeln\n",
    "pro Jahr. Aufgrund einer Limitation der Api, die nur maximal 200 Artikel pro Anfrage zurückliefert, werden mehrere Iterationen durchgeführt. Für jedes Jahr wird eine Suche durchgeführt und\n",
    "die Metadaten Titel, Autoren, Zusammenfassung und Veröffentlichungsdatum der einzelnen Artikel einem Array hinzugefügt. Zuletzt werden die Artikel in einem Dataframe gespeichert.\n"
   ],
   "id": "3eaccebc836882e6"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from arxiv import SortCriterion, Search\n",
    "\n",
    "# Variables\n",
    "articles = []\n",
    "years = [2018, 2019, 2020, 2021]\n",
    "number_of_articles = 200\n",
    "client = arxiv.Client()\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Define search query\n",
    "    search_query = f\"Deep Reinforcement Learning AND submittedDate:[{year}01010000 TO {year}12312359]\"\n",
    "\n",
    "    # Search for the articles\n",
    "    search = Search(\n",
    "        query=search_query,\n",
    "        max_results=number_of_articles,\n",
    "        sort_by=SortCriterion.SubmittedDate,\n",
    "    )\n",
    "\n",
    "    # Request the articles\n",
    "    results = client.results(search)\n",
    "\n",
    "    # Save retrieved data in array\n",
    "    for result in results:\n",
    "        article = {\n",
    "            \"Title\": result.title,\n",
    "            \"Authors\": result.authors,\n",
    "            \"Abstract\": result.summary,\n",
    "            \"PublishedDate\": result.published.date().year,\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "# Save articles in data frame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# Print types and head\n",
    "print(f\"Types of the data frame \\n {df.dtypes}\")\n",
    "print(f\"The head of the data frame \\n {df.head()}\")"
   ],
   "id": "eab8a0410836482f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vorbereiten der Daten (Data Preparation)\n",
    "\n",
    "Im folgenden Abschnitt wird eine Aufbereitung der Daten vorgenommen. Dies ist ein essenzieller Teil jedes Datenprojektes. Je besser die Daten aufbereitet wurden, desto aussagekräftiger\n",
    "sind die Ergebnisse der nachfolgenden Analyse des Datenbestands. Sie wird unmittelbar nach der Extraktion der Daten vorgenommen. Hierzu gehören üblicherweise folgende Schritte:\n",
    "\n",
    "1. Umbenennung der Spalten für eine aussagekräftigere Darstellung der Daten und des verarbeitenden Codes.\n",
    "2. Entfernung irrelevanter Spalten für die weitere Verarbeitung\n",
    "3. Entfernung von Duplikaten und Zeilen mit Null-Werten.\n",
    "4. Entfernung von Ausreißern"
   ],
   "id": "a5b0534e86fda25c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from extraction import extract_data_from_arxiv\n",
    "\n",
    "# Extract data from api\n",
    "df = extract_data_from_arxiv()\n",
    "\n",
    "# Expand list to entries\n",
    "df = df.explode(column=\"Authors\")\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Filter for year\n",
    "    year_filtered_df = df[df[\"PublishedDate\"].isin([year])]\n",
    "\n",
    "    # Counts authors for the selected year (2018-2021) and take the 10 authors with the most publications\n",
    "    top_author_counts = year_filtered_df[\"Authors\"].value_counts().head(10)\n",
    "\n",
    "    top_author_counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"Top 10 authors across {year}\")\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.yticks(range(0, 11, 1))\n",
    "    plt.show()"
   ],
   "id": "cfb3d6d51c876dbc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
