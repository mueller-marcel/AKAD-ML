{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analyse von Arxiv: Strukturierte und unstrukturierte Daten\n",
    "\n",
    "Dieses Jupyter Notebook stellt die Prüfungsaufgabe des Moduls \"Maschinelles Lernen\" (ROB60) der Akad University dar.\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Die Aufgabenstellung dieser Prüfung ist es Artikel zu einem selbstgewählten Thema der Webseite [Arxiv](https://arxiv.org/) zu analysieren.\n",
    "Im ersten Schritt wird ein Histogramm der über die Anzahl der Artikel pro Autor erstellt. Im Weiteren wird eine deskriptive Analyse für die verschiedenen Jahr durchgeführt.\n",
    "Das Ziel ist zu eruieren, ob sich über die Jahre gewisse Parameter oder Korrelationen verändert haben. Dies könnte beispielsweise die Anzahl der veröffentlichten Artikel pro Autor\n",
    "oder die Anzahl an Autoren sein. Darüber hinaus wird eine Textanalyse durchgeführt. Diese Analyse wird anhand des Abstracts der Artikel durchgeführt.\n",
    "Diese Aufgaben werden sich am CRISP DM Standard orientieren und die folgenden Punkte beinhalten:\n",
    "\n",
    "1. Verstehen der Daten (Data Understanding)\n",
    "2. Datenvorbereitung (Data Preparation)\n",
    "3. Modellierung (Modeling)\n",
    "4. Auswertung (Evaluation)\n",
    "\n",
    "## Extraktion und Verstehen der Daten (Data Understanding)\n",
    "\n",
    "Für die Extraktion der Daten findet mittels der Web-API der Platform von Arxiv statt. Es wird das exemplarische Stichwort \"Deep Reinforcement Learning\" gewählt.\n",
    "Um eine möglichst aussagekräftige Analyse durchführen zu können, werden Artikel für die Jahre 2017-2021 heruntergeladen. Pro Jahr sollen 200 Artikel heruntergeladen und in die\n",
    "Analyse miteinbezogen werden.\n",
    "\n",
    "### Beschreibung der Datenquelle und des Codes zur Extraktion\n",
    "\n",
    "Die Platform Arxiv bietet eine Web-Api an um Metadaten der Artikel zu extrahieren. Hierbei wird eine Anfrage an den Server mittels des Protokolls gesendet.\n",
    "Die Parametrisierung findet mittels der URL statt. Für die Extraktion eben jener Daten wird das Paket `arxiv` verwendet. Es bietet eine API zur Parametrisierung der Anfrage an.\n",
    "Im folgenden Code wird zunächst ein leeres Array für die extrahierten Artikel angelegt. Darüber hinaus werden die Jahreszahlen in einem Array definiert und die maximale Anzahl an Artikeln\n",
    "pro Jahr. Aufgrund einer Limitation der Api, die nur maximal 200 Artikel pro Anfrage zurückliefert, werden mehrere Iterationen durchgeführt. Für jedes Jahr wird eine Suche durchgeführt und\n",
    "die Metadaten Titel, Autoren, Zusammenfassung und Veröffentlichungsdatum der einzelnen Artikel einem Array hinzugefügt. Zuletzt werden die Artikel in einem Dataframe gespeichert.\n"
   ],
   "id": "3eaccebc836882e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from arxiv import SortCriterion, Search\n",
    "\n",
    "# Variables\n",
    "articles = []\n",
    "years = [2018, 2019, 2020, 2021]\n",
    "number_of_articles = 200\n",
    "client = arxiv.Client()\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    # Define search query\n",
    "    search_query = f\"Deep Reinforcement Learning AND submittedDate:[{year}01010000 TO {year}12312359]\"\n",
    "\n",
    "    # Search for the articles\n",
    "    search = Search(\n",
    "        query=search_query,\n",
    "        max_results=number_of_articles,\n",
    "        sort_by=SortCriterion.SubmittedDate,\n",
    "    )\n",
    "\n",
    "    results = client.results(search)\n",
    "\n",
    "    # Save retrieved data in array\n",
    "    for result in results:\n",
    "        article = {\n",
    "            \"Title\": result.title,\n",
    "            \"Authors\": result.authors,\n",
    "            \"Abstract\": result.summary,\n",
    "            \"PublishedDate\": result.published,\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "# Save articles in data frame\n",
    "df = pd.DataFrame(articles)\n",
    "print(df.head())"
   ],
   "id": "eab8a0410836482f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
